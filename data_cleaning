

import csv
import pandas as pd
import io
from pyspark.sql.functions import col

# Filter the new_files_kunde DataFrame to get the list of new CSV files
new_csv_files = [row.filePath.replace("dbfs:", "") for row in new_files_kunde.select("filePath").collect()]

data = []
header = None

for idx, file_name in enumerate(new_csv_files):
    file_path = f"{file_name}"
    with open(file_path) as f_input:
        for line_num, line in enumerate(f_input):
            # Clean the line and replace double quotes
            line = line.strip('\n').replace('""', '"')
            # Read the line as a CSV row
            row = next(csv.reader(io.StringIO(line, newline="")))
            # Set header from the first line of the first file
            if idx == 0 and line_num == 0 and header is None:
                header = row
            # Append row to data if it's not the first line of subsequent files
            elif idx == 0 or line_num > 0:
                if len(row) == len(header): data.append(row)

cleaned_df = pd.DataFrame(data, columns=header)
display("none" if cleaned_df.empty else cleaned_df)


  # Storing cleaned_df as a csv
cleaned_df.to_csv('/Volumes/.../.../cleaning_bronze_kunde/cleaned_df.csv', index=False)


  
